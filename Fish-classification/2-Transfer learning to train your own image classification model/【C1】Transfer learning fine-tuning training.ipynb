{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 忽略烦人的红色提示\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 获取计算硬件\n",
    "# 有 GPU 就用 GPU，没有就用 CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 训练集图像预处理：缩放裁剪、图像增强、转 Tensor、归一化\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "# 测试集图像预处理-RCTN：缩放、裁剪、转 Tensor、归一化\n",
    "test_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(\n",
    "                                         mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入图像分类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集文件夹路径\n",
    "dataset_dir = '/media/zy/600b4f4d-dc6c-4b45-8c58-476ec9afb3fb/cyy/work/图像分类/ newcutsave_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集路径 /media/zy/600b4f4d-dc6c-4b45-8c58-476ec9afb3fb/cyy/work/图像分类/ newcutsave_split/train\n",
      "测试集路径 /media/zy/600b4f4d-dc6c-4b45-8c58-476ec9afb3fb/cyy/work/图像分类/ newcutsave_split/val\n",
      "训练集图像数量 16000\n",
      "类别个数 4\n",
      "各类别名称 ['medium', 'none', 'strong', 'weak']\n",
      "测试集图像数量 4000\n",
      "类别个数 4\n",
      "各类别名称 ['medium', 'none', 'strong', 'weak']\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(dataset_dir, 'train')\n",
    "test_path = os.path.join(dataset_dir, 'val')\n",
    "print('训练集路径', train_path)\n",
    "print('测试集路径', test_path)\n",
    "\n",
    "from torchvision import datasets\n",
    "# 载入训练集\n",
    "train_dataset = datasets.ImageFolder(train_path, train_transform)\n",
    "# 载入测试集\n",
    "test_dataset = datasets.ImageFolder(test_path, test_transform)\n",
    "\n",
    "print('训练集图像数量', len(train_dataset))\n",
    "print('类别个数', len(train_dataset.classes))\n",
    "print('各类别名称', train_dataset.classes)\n",
    "print('测试集图像数量', len(test_dataset))\n",
    "print('类别个数', len(test_dataset.classes))\n",
    "print('各类别名称', test_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别和索引号 映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各类别名称\n",
    "class_names = train_dataset.classes\n",
    "n_class = len(class_names)\n",
    "# 映射关系：类别 到 索引号\n",
    "train_dataset.class_to_idx\n",
    "# 映射关系：索引号 到 类别\n",
    "idx_to_labels = {y:x for x,y in train_dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'medium', 1: 'none', 2: 'strong', 3: 'weak'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据加载器DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 训练集的数据加载器\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )\n",
    "\n",
    "# 测试集的数据加载器\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入训练需使用的工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择迁移学习训练方式\n",
    "\n",
    "斯坦福CS231N【迁移学习】中文精讲：https://www.bilibili.com/video/BV1K7411W7So\n",
    "\n",
    "斯坦福CS231N【迁移学习】官方笔记：https://cs231n.github.io/transfer-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'DenseNet',\n",
       " 'EfficientNet',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'MNASNet',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'RegNet',\n",
       " 'ResNet',\n",
       " 'ShuffleNetV2',\n",
       " 'SqueezeNet',\n",
       " 'VGG',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'feature_extraction',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择一：只微调训练模型最后一层（全连接分类层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.AlexNet(pretrained=True) # 载入预训练模型\n",
    "# 修改全连接层，使得全连接层的输出与当前数据集类别数对应\n",
    "# 新建的层默认 requires_grad=True\n",
    "# model.fc = nn.Linear(model.fc.in_features, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg11(num_classes=4).to(device)\n",
    "# model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier\n",
    "# model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只微调训练最后一层全连接层的参数，其它层冻结\n",
    "# # optimizer = optim.SGD(resnet18_model.parameters(),lr=LR,momentum=0.9)\n",
    "# optimizer = optim.Adam(model.fc.parameters(),lr=0.001)\n",
    "optimizer = optim.Adam(model.classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择二：微调训练所有层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet34(pretrained=True) # 载入预训练模型\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, n_class)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择三：随机初始化模型全部权重，从头训练所有层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.AlexNet(pretrained=False) # 只载入模型结构，不载入预训练权重参数\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, n_class)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device=torch.device(\"cuda:1\" )\n",
    "model = model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# 训练轮次 Epoch\n",
    "EPOCHS = 30\n",
    "\n",
    "# 学习率降低策略\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数：在训练集上训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(images, labels):\n",
    "    '''\n",
    "    运行一个 batch 的训练，返回当前 batch 的训练日志\n",
    "    '''\n",
    "    \n",
    "    # 获得一个 batch 的数据和标注\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = model(images) # 输入模型，执行前向预测\n",
    "    loss = criterion(outputs, labels) # 计算当前 batch 中，每个样本的平均交叉熵损失函数值\n",
    "    \n",
    "    # 优化更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 获取当前 batch 的标签类别和预测类别\n",
    "    _, preds = torch.max(outputs, 1) # 获得当前 batch 所有图像的预测类别\n",
    "    preds = preds.cpu().numpy()\n",
    "    loss = loss.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    \n",
    "    log_train = {}\n",
    "    log_train['epoch'] = epoch\n",
    "    log_train['batch'] = batch_idx\n",
    "    # 计算分类评估指标\n",
    "    log_train['train_loss'] = loss\n",
    "    log_train['train_accuracy'] = accuracy_score(labels, preds)\n",
    "    # log_train['train_precision'] = precision_score(labels, preds, average='macro')\n",
    "    # log_train['train_recall'] = recall_score(labels, preds, average='macro')\n",
    "    # log_train['train_f1-score'] = f1_score(labels, preds, average='macro')\n",
    "    \n",
    "    return log_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数：在整个测试集上评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset():\n",
    "    '''\n",
    "    在整个测试集上评估，返回分类评估指标日志\n",
    "    '''\n",
    "\n",
    "    loss_list = []\n",
    "    labels_list = []\n",
    "    preds_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader: # 生成一个 batch 的数据和标注\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images) # 输入模型，执行前向预测\n",
    "\n",
    "            # 获取整个测试集的标签类别和预测类别\n",
    "            _, preds = torch.max(outputs, 1) # 获得当前 batch 所有图像的预测类别\n",
    "            preds = preds.cpu().numpy()\n",
    "            loss = criterion(outputs, labels) # 由 logit，计算当前 batch 中，每个样本的平均交叉熵损失函数值\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "\n",
    "            loss_list.append(loss)\n",
    "            labels_list.extend(labels)\n",
    "            preds_list.extend(preds)\n",
    "        \n",
    "    log_test = {}\n",
    "    log_test['epoch'] = epoch\n",
    "    \n",
    "    # 计算分类评估指标\n",
    "    log_test['test_loss'] = np.mean(loss)\n",
    "    log_test['test_accuracy'] = accuracy_score(labels_list, preds_list)\n",
    "    log_test['test_precision'] = precision_score(labels_list, preds_list, average='macro')\n",
    "    log_test['test_recall'] = recall_score(labels_list, preds_list, average='macro')\n",
    "    log_test['test_f1-score'] = f1_score(labels_list, preds_list, average='macro')\n",
    "    \n",
    "    return log_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练开始之前，记录日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "batch_idx = 0\n",
    "best_test_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练日志-训练集\n",
    "df_train_log = pd.DataFrame()\n",
    "log_train = {}\n",
    "log_train['epoch'] = 0\n",
    "log_train['batch'] = 0\n",
    "images, labels = next(iter(train_loader))\n",
    "log_train.update(train_one_batch(images, labels))\n",
    "df_train_log = df_train_log.append(log_train, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>1.3983223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch  epoch  train_accuracy train_loss\n",
       "0    0.0    0.0         0.21875  1.3983223"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练日志-测试集\n",
    "df_test_log = pd.DataFrame()\n",
    "log_test = {}\n",
    "log_test['epoch'] = 0\n",
    "log_test.update(evaluate_testset())\n",
    "df_test_log = df_test_log.append(log_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1-score</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.162862</td>\n",
       "      <td>11.444318</td>\n",
       "      <td>0.179908</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  test_accuracy  test_f1-score  test_loss  test_precision  test_recall\n",
       "0    0.0          0.282       0.162862  11.444318        0.179908        0.282"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 登录wandb\n",
    "\n",
    "1.安装 wandb：pip install wandb\n",
    "\n",
    "2.登录 wandb：在命令行中运行wandb login\n",
    "\n",
    "3.按提示复制粘贴API Key至命令行中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建wandb可视化项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: cyyzm (711). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/zy/600b4f4d-dc6c-4b45-8c58-476ec9afb3fb/cyy/work/图像分类/3-【Pytorch】迁移学习训练自己的图像分类模型/wandb/run-20231229_211333-1wkyli5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/711/new%20fish/runs/1wkyli5c\" target=\"_blank\">1229211331</a></strong> to <a href=\"https://wandb.ai/711/new%20fish\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/711/new%20fish/runs/1wkyli5c?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc5ce015208>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='new fish ', name=time.strftime('%m%d%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存新的最佳模型 checkpoints/best-0.000.pth\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:51<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存新的最佳模型 checkpoints/best-0.937.pth\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:51<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:51<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:53<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:52<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    \n",
    "    ## 训练阶段\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_loader): # 获得一个 batch 的数据和标注\n",
    "        batch_idx += 1\n",
    "        log_train = train_one_batch(images, labels)\n",
    "        df_train_log = df_train_log.append(log_train, ignore_index=True)\n",
    "        wandb.log(log_train)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "\n",
    "    ## 测试阶段\n",
    "    model.eval()\n",
    "    log_test = evaluate_testset()\n",
    "    df_test_log = df_test_log.append(log_test, ignore_index=True)\n",
    "    wandb.log(log_test)\n",
    "    \n",
    "    # 保存最新的最佳模型文件\n",
    "    if log_test['test_accuracy'] > best_test_accuracy: \n",
    "        # 删除旧的最佳模型文件(如有)\n",
    "        old_best_checkpoint_path = 'checkpoints/best-{:.3f}.pth'.format(best_test_accuracy)\n",
    "        if os.path.exists(old_best_checkpoint_path):\n",
    "            os.remove(old_best_checkpoint_path)\n",
    "        # 保存新的最佳模型文件\n",
    "        new_best_checkpoint_path = 'checkpoints/best-{:.3f}.pth'.format(log_test['test_accuracy'])\n",
    "        torch.save(model, new_best_checkpoint_path)\n",
    "        print('保存新的最佳模型', 'checkpoints/best-{:.3f}.pth'.format(best_test_accuracy))\n",
    "        best_test_accuracy = log_test['test_accuracy']\n",
    "\n",
    "df_train_log.to_csv('训练日志-训练集.csv', index=False)\n",
    "df_test_log.to_csv('训练日志-测试集.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在测试集上评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入最佳模型作为当前模型\n",
    "model = torch.load('checkpoints/best-{:.3f}.pth'.format(best_test_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 30, 'test_loss': 0.21541001, 'test_accuracy': 0.941, 'test_precision': 0.9433083176985615, 'test_recall': 0.9410000000000001, 'test_f1-score': 0.9412832734864098}\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "print(evaluate_testset())\n",
    "# torch.save(model, 'checkpoints/new fish_pytorch_squeezenet1_0 n.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvison import models\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     resnet50 = models.resnet50().cuda()\n",
    "#     summary(resnet50, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "flops:  15218280448.0 params:  132863336.0\n",
      "flops: 15218.28 M, params: 132.86 M\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "model = torchvision.models.vgg11(pretrained=False)\n",
    "\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "flops, params = profile(model, (dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7a9bb930f606a8a9200d8832eafe4bf9489fee47d8f93830a3febf35c93c08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
